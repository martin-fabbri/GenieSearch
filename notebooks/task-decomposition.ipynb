{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --q langchain-community\n",
    "%pip install --q langchain\n",
    "%pip install --q duckduckgo-search\n",
    "\n",
    "# TODO: add the following packages to poetry\n",
    "%pip install --q requests\n",
    "%pip install --q beautifulsoup4\n",
    "\n",
    "#################################\n",
    "# Required for PaperSpace Gradient\n",
    "%pip install --q typing-inspect==0.8.0 typing_extensions==4.5.0\n",
    "%pip install --q pydantic==1.10.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   \tID          \tSIZE  \tMODIFIED       \n",
      "mixtral:instruct       \t7708c059a8bb\t26 GB \t17 minutes ago\t\n",
      "nomic-embed-text:latest\t0a109f422b47\t274 MB\t27 minutes ago\t\n"
     ]
    }
   ],
   "source": [
    "# %> curl -fsSL https://ollama.com/install.sh | sh\n",
    "# %> ollama serve\n",
    "# %> ollama pull nomic-embed-text\n",
    "# %> ollama pull mixtral:instruct     (gemma:7b-instruct | mistral:instruct)\n",
    "\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerator Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 27 14:06:00 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   41C    P8    31W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{text}\n",
    "-----------\n",
    "Using the above text, answer in short the following question:\n",
    "> {question}\n",
    "-----------\n",
    "if the question cannot be answered using the text, imply summarize the text. Include all\n",
    "factual information, numbers, stats etc if available.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_url(url: str):\n",
    "    try:\n",
    "        # set a get request to webpage\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # if the request response was successful\n",
    "        if (response.status_code == 200):\n",
    "            # parse the content of the request\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            text_content = soup.get_text(separator=\" \", strip=True)\n",
    "            return text_content\n",
    "        else:\n",
    "            return f\"Failed to retrieve page: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(e)    \n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://blog.langchain.dev/announcing-langsmith/\"\n",
    "\n",
    "content = scrape_url(url)[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Local LLM & Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = \"mixtral:instruct\"  # (\"gemma:7b-instruct\" | \"mistral:instruct\")\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "TEMPERATURE = 0.9\n",
    "ENABLE_TRACING = False\n",
    "DOCUMENT_CHUNK_SIZE = 7500  # Gemma --> DOCUMENT_CHUNK_SIZE=5000\n",
    "CHUNK_OVERLAP = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Test Local LLM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Ollama model. No need to import separate llm model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I am a large language model trained by Mistral AI. I was designed to answer questions and assist with tasks in a conversational manner, such as giving explanations or providing suggestions.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "_test_llm_model_ = Ollama(\n",
    "    model=LLM_MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    ")\n",
    "\n",
    "_test_llm_model_(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Chat Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' LangSmith is a unified platform for developing, debugging, testing, evaluating, and monitoring large language model (LLM) applications. It provides a framework for creating chains of LLMs and prompts, as well as tools for logging, tracing, and visualizing the inputs and outputs of these chains. LangSmith also includes features for creating datasets from traces or manual curation, running chains and prompts over those data sets, and evaluating their performance using heuristics or LLMs. It integrates seamlessly with open source evaluation modules and can be used to monitor system-level performance, track model/chain performance, debug issues, and understand user interactions. LangSmith is designed to help developers move from prototype to production more efficiently and effectively, and to provide a single hub for managing LLM applications.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chain = prompt | ChatOllama(model=LLM_MODEL) | StrOutputParser()\n",
    "chat_response = chain.invoke(\n",
    "    {\n",
    "        \"text\": content,\n",
    "        \"question\": \"What is LangSmith?\"\n",
    "    }\n",
    ")\n",
    "chat_response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
