{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytest 7.2.1 requires attrs>=19.2.0, but you have attrs 18.2.0 which is incompatible.\n",
      "gradient 2.0.6 requires marshmallow<3.0, but you have marshmallow 3.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires marshmallow<3.0, but you have marshmallow 3.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spacy 3.4.1 requires pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4, but you have pydantic 1.10.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --q langchain-community\n",
    "%pip install --q langchain\n",
    "%pip install --q duckduckgo-search\n",
    "\n",
    "# TODO: add the following packages to poetry\n",
    "%pip install --q requests\n",
    "%pip install --q beautifulsoup4\n",
    "\n",
    "#################################\n",
    "# Required for PaperSpace Gradient\n",
    "%pip install --q typing-inspect==0.8.0 typing_extensions==4.5.0\n",
    "%pip install --q pydantic==1.10.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                   \tID          \tSIZE  \tMODIFIED      \n",
      "mixtral:instruct       \t7708c059a8bb\t26 GB \t6 seconds ago\t\n",
      "nomic-embed-text:latest\t0a109f422b47\t274 MB\t4 minutes ago\t\n"
     ]
    }
   ],
   "source": [
    "# %> curl -fsSL https://ollama.com/install.sh | sh\n",
    "# %> ollama serve\n",
    "# %> ollama pull nomic-embed-text     ???? do we need a retriever???\n",
    "# %> ollama pull mixtral:instruct     (gemma:7b-instruct | mistral:instruct)\n",
    "\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerator Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 27 22:04:19 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:00:05.0 Off |                  Off |\n",
      "| 30%   46C    P8    21W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"{text}\n",
    "-----------\n",
    "Using the above text, answer in short the following question:\n",
    "> {question}\n",
    "-----------\n",
    "if the question cannot be answered using the text, imply summarize the text. Include all\n",
    "factual information, numbers, stats etc if available.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_url(url: str):\n",
    "    try:\n",
    "        # set a get request to webpage\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # if the request response was successful\n",
    "        if (response.status_code == 200):\n",
    "            # parse the content of the request\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            text_content = soup.get_text(separator=\" \", strip=True)\n",
    "            return text_content\n",
    "        else:\n",
    "            return f\"Failed to retrieve page: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        print(e)    \n",
    "\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://blog.langchain.dev/announcing-langsmith/\"\n",
    "\n",
    "content = scrape_url(url)[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Local LLM & Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_MODEL = \"mixtral:instruct\"  # (\"gemma:7b-instruct\" | \"mistral:instruct\")\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "TEMPERATURE = 0.9\n",
    "ENABLE_TRACING = False\n",
    "DOCUMENT_CHUNK_SIZE = 7500  # Gemma --> DOCUMENT_CHUNK_SIZE=5000\n",
    "CHUNK_OVERLAP = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load & Test Local LLM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Ollama model. No need to import separate llm model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' I am a language model trained by the Mistral AI team. I generate text based on the input I receive, and my purpose is to provide useful, safe, and accurate information.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "\n",
    "_test_llm_model_ = Ollama(\n",
    "    model=LLM_MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    ")\n",
    "\n",
    "_test_llm_model_(\"Who are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Search Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "NUM_RESULTS_PER_QUESTION = 3\n",
    "ddg_search = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "async def web_search(query: str, num_results: int = NUM_RESULTS_PER_QUESTION):\n",
    "    web_query_results = ddg_search.results(query, num_results)\n",
    "    return [r[\"link\"] for r in web_query_results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://blog.langchain.dev/announcing-langsmith/',\n",
       " 'https://blog.logrocket.com/langsmith-test-llms-ai-applications/',\n",
       " 'https://dev.to/logankilpatrick/what-is-langsmith-and-why-should-i-care-as-a-developer-19k']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await web_search(\"What is langsmith?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Chat Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' LangSmith is a platform that allows developers to create and manage language models for various use cases. It provides a user-friendly interface and tools to build, train, and deploy custom language models without requiring extensive knowledge of machine learning or natural language processing. Additionally, LangSmith offers integration with popular programming languages such as Python and allows users to create components using the LangChain library. These components can be used for various tasks, including chatbots, virtual assistants, and data analysis. The platform also provides a waitlist for access and requires API keys for authentication and authorization purposes.',\n",
       " ' LangSmith is a debugging, testing, and monitoring platform for large language models (LLMs) built on top of the open-source LangChain framework. It provides a unified interface for developers to easily visualize the exact sequence of calls, inputs, and outputs at each step in the chain, making it easier to build intuition as they learn to create new and more sophisticated applications. LangSmith also integrates seamlessly with an open source collection of evaluation modules for heuristic and LLM evaluations, and allows developers to track system-level performance, model/chain performance, debug issues, and understand user interactions with their application. The platform is already being used by companies such as Klarna, BCG, Mendable, Multi-On, Quivr, and Theodo for various purposes including debugging, testing, evaluating, and monitoring LLM applications in production.',\n",
       " ' LangSmith is a tool that allows developers to trace and evaluate their Language Learning Models (LLMs) applications by providing an API key for access and configuring the runtime environment using shell commands. It also offers a Cookbook repository containing real-world use-cases, examples, and patterns of LLM applications to inspire and assist developers in their projects. The Cookbook is community-driven, meaning that users can contribute their insights and use-cases. Key examples from the Cookbook include tracing your code, using REST API features, customizing run names, testing and evaluation, and exporting data for fine-tuning LLMs.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "scrape_and_summarize_chain = (\n",
    "    RunnablePassthrough.assign(text=lambda x: scrape_url(x[\"url\"])[:10000])\n",
    "    | prompt\n",
    "    | ChatOllama(model=LLM_MODEL)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "web_search_chain = (\n",
    "    RunnablePassthrough.assign(urls=lambda x: asyncio.run(web_search(x[\"question\"])))\n",
    "    | (lambda x: [{\"question\": x[\"question\"], \"url\": u} for u in x[\"urls\"]])\n",
    "    | scrape_and_summarize_chain.map()\n",
    ")\n",
    "\n",
    "\n",
    "chat_response = web_search_chain.invoke({\"question\": \"What is LangSmith?\"})\n",
    "chat_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Web Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['langchain definition',\n",
       " 'objective review of langchain',\n",
       " 'expert opinions on langchain']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "questions_generation = (\n",
    "    \"Write 3 google search queries to search online that form an objective \"\n",
    "    \"opinion from the following question: {question}\\n \"\n",
    "    \"Return a python list in the following format: \"\n",
    "    \"['query 1', 'query 2', 'query 3']\\n \"\n",
    "    \"Do not include anything else after the list.\"\n",
    ")\n",
    "\n",
    "SEARCH_PROMPT = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"user\",\n",
    "            questions_generation,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "search_qanda_chain = (\n",
    "    SEARCH_PROMPT \n",
    "    | ChatOllama(model=LLM_MODEL) \n",
    "    | StrOutputParser()\n",
    "    | eval \n",
    ")\n",
    "\n",
    "search_qanda_chain.invoke({\n",
    "    \"question\": \"What is langchain?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Write 3 google search queries to search online that form an objective opinion from the following question: {question}\\n Return a python list in the following format: ['query 1', 'query 2', 'query 3']. Do not include anything else.\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
